# 5章 リカレントニューラルネットワーク（RNN）
これまで見てきたニューラルネットワーク（word2vecなど）　＝　フィードフォワードネットワーク
- 信号の流れが一方向。構成が単純で理解しやすく、多くの問題に応用できる。
- しかし、時系列データを扱うことができない（時系列データの性質を十分に学習できない）

`リカレントニューラルネットワーク`（略して`RNN`）を使えば時系列データを扱うことができる
- この章では、フィードフォワードネットワークの問題点を指摘し、RNNがその問題を解決できることを説明する

## 5.1 確率と言語モデル
5.1の内容は以下のとおり
- word2vecを復習する
- 自然言語に関する現象を「確率」を使って記述する
- 言語を確率として扱う「言語モデル」を説明する

### 5.1.1 word2vecを確率の視点から眺める
word2vecのCBOWモデル（Continuous Bag-of-Words Model）の復習

- T個の単語からなるコーパスを考えて、
- t番目の単語を「ターゲット」とし、
- その前後（t-1番目とt+1番目）の単語を「コンテキスト」として扱う

上記の場合にCBOWモデルが行うことは、W<sub>t-1</sub>とW<sub>t+1</sub>からW<sub>t</sub>を推測すること。★図5-1参照★

>W<sub>1</sub> W<sub>2</sub> ・・・ W<sub>t-1</sub> W<sub>t</sub> W<sub>t+1</sub> ・・・ W<sub>T-1</sub> W<sub>T</sub>

例えば、以下の???をyouとgoodbyeから推測することができる。

>you <span style="padding: 1px; margin-bottom: 1px; border: 1px solid #333333;">???</span> goodbye and I say hello.

数式で書くと以下になる。この事後確率は「w[t-1]とw[t+1]が与えられたとき、w[t]が起こる確率」を表す。これがウインドウサイズが1のCBOWモデルになる。

>P(w<sub>t</sub> | w<sub>t-1</sub> ,w<sub>t+1</sub>)<span  style="float: right">(5.1)</span>

コンテキストを左のウインドウだけに限定すると、以下となる　★図5-2参照★

>W<sub>1</sub> W<sub>2</sub> ・・・ W<sub>t-1</sub> W<sub>t</sub> W<sub>t+1</sub> ・・・ W<sub>T-1</sub> W<sub>T</sub>

左側2つの単語だけをコンテキストとして考えると、CBOWモデルが出力する確率は以下の式になる。

>P(w<sub>t</sub> | w<sub>t-2</sub>, w<sub>t-1</sub>)<span  style="float: right">(5.2)</span>

例えば、以下の???をyouとsayから推測する。

>you say <span style="padding: 1px; margin-bottom: 1px; border: 1px solid #333333;">???</span> and I say hello.

この場合、CBOWモデルが扱う損失関数は以下のように表現できる
- この数式は交差エントロピー誤差から導かれた結果とのこと
- ★1.3.1章を要確認★

>L = -log P(w<sub>t</sub> | w<sub>t-2</sub>, w<sub>t-1</sub>)<span  style="float: right">(5.3)</span>

CBOWモデルの学習で行うことは、上の式で表される損失関数をできる限り小さくする重みパラメータを見つけること。

そのような重みパラメータが見つかれば、CBOWモデルはコンテキストからターゲットをより正しく推測できる
- これがCBOWモデルの学習の本来の目的。「コンテキストからターゲットを推測すること」は「言語モデル」で利用できる。
  - ★こういう意味で合っているか？★

### 5.1.2 言語モデル
`言語モデル`（Language Model）は単語の並びに対して確率を与える
- ある単語の並びがどれだけ自然かを確率で評価する
  - 「you say goodbye」には高い確率（たとえば0.092）を出力
  - 「you say good die」には低い確率（たとえば0.000000000032）を出力

言語モデルはさまざまなアプリケーションで利用できる
- 機械翻訳や音声認識 ： 候補の文章が複数生成された場合に、「文章として自然かどうか」という基準でランク付けできる
- 文章生成 ： 確率分布に従って単語を紡ぎだす（サンプリングする）ことができる
  - 言語モデルを使った文章生成は7章で解説

数式を使った言語モデルの記述
- w<sub>1</sub>, ... ,w<sub>m</sub>というm個の単語からなる分章について考える
- 単語の出現確率は P(w<sub>1</sub>, ..., w<sub>m</sub>) で表される
  - 複数の事象が同時に起こる確率のため同時確率と呼ばれる

同時確率は、「確率の乗法定理」を当てはめると、事後確率を使って以下のように書くことができる
- 確率の乗法定理 ： `P(A,B) = P(A|B) P(B)` または `P(A,B) = P(B|A) P(A)`
- 「AとBの両方が起こる確率」は「Bが起こる確率」と「Bが起こった後にAが起こる確率」を掛け合わせたものになる


>P(w<sub>1</sub>, ..., w<sub>m</sub>) = P(w<sub>m</sub> | w<sub>1</sub>, ..., w<sub>m-1</sub>) P(w<sub>m-1</sub> | w<sub>1</sub>, ..., w<sub>m-2</sub>) ... P(w<sub>3</sub> | w<sub>1</sub>, w<sub>2</sub>) P(w<sub>2</sub> | w<sub>1</sub>) P(w<sub>1</sub>)
<br/><img src="https://latex.codecogs.com/gif.latex?=\prod_{t=1}^{m} {P(w_t | w_1, ..., w_{t-1})}" /><span  style="float: right">(5.4)</span>
>>「w<sub>1</sub>とw<sub>2</sub>と・・・w<sub>m</sub>が同時に起こる確率」＝
<br/>「w<sub>1</sub>が起こる確率」×
<br/>「w<sub>1</sub>が起こった後にw<sub>2</sub>が起こる確率」×
<br/>「w<sub>1</sub>とw<sub>2</sub>が起こった後にw<sub>3</sub>が起こる確率」×
<br/>・・・・・・×
<br/>「w<sub>1</sub>とw<sub>2</sub>と・・・・w<sub>m-1</sub>が起こった後にw<sub>m</sub>が起こる確率」

上記の式の事後確率の部分は、対象の単語より左側のすべての単語をコンテキストとしたときの確率である　★図5-3参照★
- P(w<sub>3</sub> | w<sub>1</sub>, w<sub>2</sub>)は、w<sub>1</sub>, w<sub>2</sub>をコンテキストとしたときの確率
- P(w<sub>m</sub> | w<sub>1</sub>, ..., w<sub>m-1</sub>)は、w<sub>1</sub>からw<sub>m-1</sub>をコンテキストとしたときの確率

→　この確率を求めることができれば、言語モデルの同時確率P(w<sub>1</sub>, ..., w<sub>m</sub>)を求めることができる
  
### 5.1.3 CBOWモデルを言語モデルに？

word2vecのCBOWモデルでコンテキストのサイズをある値に限定すれば、近似的に言語モデルに適用できる（無理やり）

>P(w<sub>1</sub>, ..., w<sub>m</sub>)
<br/><img src="https://latex.codecogs.com/gif.latex?=\prod_{t=1}^{m} {P(w_t | w_1, ..., w_{t-1})}\approx\prod_{t=1}^{m} {P(w_t | w_{t-2}, w_{t-1})}" /><span  style="float: right">(5.8)</span>
>>ここではコンテキストを左側2つの単語に限定している
>>>このモデルは直前の2個の単語だけに依存して次の単語が決まるため、「2階マルコフ連鎖」と呼ぶことができる

CBOWモデルでは、コンテキストのサイズは任意の長さに設定できる（たとえば5や10など）が、長さは固定しないければならない　★なぜか？★

しかし、コンテキストよりも左側の単語が無視されるという問題がある
>例：Tom was  watching TV in his room. Mary came into the room. Mary said hi to <span style="padding: 1px; margin-bottom: 1px; border: 1px solid #333333;">???</span>.

という文章で「???」に入る単語を予測するには18個前の「Tom」という単語を記憶しておく必要がある
- では、コンテキストのサイズを大きくしたら良い？　→　CBOWモデルではコンテキスト内の単語の並びが無視される
  - CBOWモデルの中間層は単語ベクトルの和が求められる
    - ★図5-5参照★
    - たとえば、`(you, say)`と`(say, you)`というコンテキストが同じものとして扱われる
- では単語の並びを表現するために中間層を「連結」する？　→　コンテキストのサイズに比例して重みパラメータが増える
  - そういったパラメータの増加は歓迎されない
    - ★計算量が増えたり、うまく学習が進まなくなるリスクがある？★

これらの問題を解決するのが`リカレントニューラルネットワーク`、略して`RNN`
  - RNNはどれだけコンテキストが長くても、その情報を記憶するメカニズムを持つ
  - どんなに長い時系列データでも扱うことができる

>注：実際はword2vec（2013）よりRNN（2010）のほうが早く提案されている。この書籍ではRNNの魅力を引き出すために、無理やりword2vecのCBOWモデルを言語モデルに適用しようとするときの問題を示した、とのこと。
>>word2vecは単語の分散表現を獲得することを目的に考案された手法のため、言語モデルとして使用されることは通常はない

## 5.2 RNNとは
- リカレントニューラルネットワーク（Recurrent Neural Network）の略
- `Recurrent`はラテン語からきた言葉で「何度も繰り返し起こること」を意味する
- RNNを直訳すると「循環するニューラルネットワーク」
  - 実際は「再起ニューラルネットワーク」と訳されるのが一般的
  - Recursive Neural Networkという種類のネットワークもあるが別物なので注意

### 5.2.1 循環するニューラルネットワーク
- 「循環する」＝「繰り返し回り続ける」
- 循環するためには「閉じた経路（ループする経路）」が必要
  - 「閉じた経路」が存在することで初めて、データは同じ場所を繰り返し行き来することができる
  - cf.血液は体内を循環しており、絶え間なく"更新"され続けている

RNNで用いられるレイヤを「RNNレイヤ」と呼ぶことにする

RNNレイヤはループする経路を持つ。★図5-6参照★
- 図5-6は、(x<sub>0</sub>, x<sub>1</sub>, ..., x<sub>t</sub>, ...)というデータが入力され、(h<sub>0</sub>, h<sub>1</sub>, ..., h<sub>t</sub>, ...)が出力されることを示している
  - RNNは同じ出力が2つに分岐して、その一つが自分自身への入力になる
- 時刻tではx<sub>t</sub>が入力される。x<sub>t</sub>は何らかのベクトルを想定する
  - 文章を扱う場合は、各単語の分散表現（単語ベクトル）をx<sub>t</sub>として、RNNレイヤに入力する

RNNのループを展開すると右方向に長く伸びたニューラルネットワークになる★図5-8参照★
- これはフィードフォワード型ニューラルネットワークと同じ構造
- ただし、RNNレイヤがすべて「同じレイヤ」であることが、これまでのニューラルネットワークと異なる
  - ★これまでは違うレイヤだった？★

各時刻のRNNレイヤは、そのレイヤへの入力とひとつ前のRNNレイヤからの出力を受け取る
  - 時系列データのインデックスは時刻と呼ぶ。自然言語の場合でも、「時刻tの単語」や「時刻tのRNNレイヤ」と表現することがある

このとき行う計算は以下のとおり

>h<sub>t</sub> = tanh( h<sub>t-1</sub>W<sub>h</sub> + x<sub>t</sub>W<sub>x</sub> + b )
>>記号の説明
>><ul>
>><li>W<sub>x</sub> : 入力xを出力hに変換するための重み</li>
>><li>W<sub>h</sub> : ひとつ前のRNNの出力を次時刻の出力に変換するための重み</li>
>><li>b : バイアス</li>
>><li>h<sub>t-1</sub>, x<sub>t</sub> : 行ベクトル（ベクトルの成分を横に並べたもの）</li>
>></ul>
>>計算式の意味
>><ul>
>><li>行列の積による計算を行い、それらの和をtanh関数（双曲線正接関数）によって変換する★なぜtanh関数なのか？★</li>
>><li>その結果が時刻tの出力h<sub>t</sub>となる</li>
>><li>h<sub>t</sub>は別のレイヤへ向けて上方向へ出力されると同時に、次時刻のRNNレイヤ（自分自身）に向けて右方向にも出力される</li>
>>  - ★上方向に出力されたものはどのレイヤに入るのか？★
>></ul>

RNNレイヤは「状態を持つレイヤ」「記憶力を持つレイヤ」であると言われる
- RNNでは現在の出力( h<sub>t</sub> )は、ひとつ前の出力( h<sub>t-1</sub> )によって計算される
- これは、RNNがhという状態を持っており、上記の数式で状態が更新されると解釈できる
  - RNNの出力h<sub>t</sub>は`隠れ状態`や`隠れ状態ベクトル`と呼ばれる

>注：本書でのRNNレイヤの図法は一般的な図法とは異なるので注意
>>出力が同じデータ（コピーして分岐されたもの）であることが分かるようにこのような記法となっている　★図5-9参照★

### 5.2.3 Backpropagation Through Time

RNNレイヤは横方向に伸びたニューラルネットワークのため、通常のニューラルネットワークと同じ手順で学習できる　★図5-10参照★
 - 通常の誤差逆伝播法を使って目的とする勾配を求めることができる
   - 最初に順伝播を行い
   - 続いて逆伝播を行う

「時間方向に展開したニューラルネットワークの誤差逆伝播法」ということで、`Backpropagation Through Time`または略して`BPTT`と呼ばれる

BPTTで学習を行うには、長い時系列データを学習する場合の対応を考える必要がある
- 時系列データの時間サイズが大きくなるに比例して、BPTTで消費するコンピュータの計算リソースが増加する
  - 各時刻のRNNレイヤの中間データをメモリに保持しておく必要があるため、時間サイズが大きくなるとメモリ使用量が増加する
- 時間サイズが長くなると、逆伝播時の勾配が不安定になる
  - 5.2.4によると、サイズが長くなるにつれて徐々に勾配が小さくなることがあり、勾配が前時刻に届かなくなることがある

### 5.2.4 Truncated BPTT
大きな時系列データを扱うときによく行われるのが、Truncated BPTT
- 適当な長さでネットワークを切り取り、小さなネットワーク複数に分ける
- 切り取った小さなネットワークに対して、誤差逆伝播法を行う
  - Truncatedとは「切り取られた」という意味

Truncated BPTTでは、ネットワークの「逆伝播」のつながりだけを断ち切る
- 順伝播のつながりは維持する　＝　順伝播の流れは途切れることなく伝播する
- 逆伝播のつながりは適当な長さで切り取る　＝　切り取られたネットワーク単位で学習を行う

具体例：1,000個の長さの時系列データ（1000個の単語が並んだコーパスに相当）を学習する場合
- RNNレイヤの長さが10個単位で学習できるように逆伝播のつながりを切断する　★図5-11参照★
  - ★10個という数字には意味はないと思われる？★
- 順伝播のつながりは切断しない
  - これまで見てきたニューラルネットワークのミニバッチ学習とは異なり、データを順番に（"シーケンシャル"に）与える必要がある

Truncated BPTTによるRNNの学習では、データをシーケンシャルに与えることで、隠れ状態を引き継ぎながら学習を行う（★図5-12、5-13、5-14参照）
- まずはじめに、ひとつ目のブロックの入力データ（ x<sub>0</sub>, ..., x<sub>9</sub> )をRNNレイヤに与え、順伝播と逆伝播を行う
  - ★図5-12の処理が行われる★
- 続いて、次のブロックの入力データ（ x<sub>10</sub>, ..., x<sub>19</sub> ）を対象に順伝播と逆伝播を行う
  - ここで重要なのが、順伝播の計算の入力として、h<sub>9</sub>（前ブロックの最後の隠れ状態）が必要である、ということ
    - これによって順伝播のつながりが維持される
- 続いて、3つ目のブロックを対象に順伝播と逆伝播を行う
  - このときも前ブロックの最後の隠れ状態 h<sub>19</sub> を利用する

このようにデータをシーケンシャルに与えることで、順伝播のつながりを維持しながら、ブロック単位で誤差逆伝播法を適用できる

### 5.2.5 Truncated BPTTのミニバッチ学習

Truncated BPTTでミニバッチ学習を行うためには、データを与える開始位置を各バッチで"ズラす"必要がある

具体例：1,000個の長さの時系列データに対して、時間の長さを10個単位で切ってTruncated BPTTで学習する場合で、バッチを2にした場合 ★図5-15参照★
- ひとつ目のバッチには、先頭から順にデータを与える
  - （x<sub>0</sub>, ..., x<sub>9</sub>）⇒（x<sub>10</sub>, ..., x<sub>19</sub>）⇒ ... ⇒（x<sub>490</sub>, ..., x<sub>499</sub>）
- ふたつ目のバッチには、500番目のデータを開始位置として、そこから順にデータを与える
  - （x<sub>500</sub>, ..., x<sub>509</sub>）⇒（x<sub>510</sub>, ..., x<sub>519</sub>）⇒ ... ⇒（x<sub>990</sub>, ..., x<sub>999</sub>）
- つまり、バッチの開始位置を`500`だけズラしている
  - ミニバッチ学習を行う場合、各バッチの開始位置をオフセットとしてズラして、シーケンシャルにデータを与えていけばよい
    - ★この例だと、499番目のデータと500番目のデータで順伝播のつながりが切れているように見えるが、RNNのミニバッチ学習はそういうものか？★

## まとめ
- RNNレイヤのループ構造は、展開すると通常のニューラルネットワークの形状になる
- RNNレイヤは通常のニューラルネットワークと同様に誤差逆伝播法で学習できる
  - 「時系列に展開したニューラルネットワークの誤差逆伝播法」ということで、`Backpropagation Through Time`略して`BPTT`と呼ばれる
- 時系列データのサイズが大きくても学習できるようにするために、ネットワークを適当な長さで断ち切る。この手法を`Truncated BPTT`と呼ぶ
  - 順伝播のつながりは維持し、逆伝播のつながりだけ切断するのがポイント
- 順伝播のつながりを維持して誤差逆伝播法を行うために、前のブロックの隠れ状態を、次のブロックに与える
  - すなわち、`「データをシーケンシャルに与える」`
- `Truncated BPTT`のミニバッチ学習を行うには、`「与えるデータの開始位置をズラす」`と良い
